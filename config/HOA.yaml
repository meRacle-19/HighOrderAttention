#data
#data format:ffm
data:
     # test_file  : data/test.ffm
     # infer_file  :  data/test.ffm
#     train_file  :  data/sample/train.ffm
#     eval_file  :  data/sample/eval.ffm
#     FEATURE_COUNT : 28651
#     train_file  :  data/sample_1000w/train.ffm
#     eval_file  :  data/sample_1000w/eval.ffm
#     FEATURE_COUNT : 522604
     train_file  :  data/sample_2000w/train.ffm
     eval_file  :  data/sample_2000w/eval.ffm
     FEATURE_COUNT : 885789
#     train_file  :  data/train.ffm
#     eval_file  :  data/eval.ffm
#     FEATURE_COUNT : 1738970
     FIELD_COUNT :  39
     data_format : ffm

#model
#model_type:deepFM or deepWide or dnn or ipnn or opnn or fm or lr
model:
    method : classification
    model_type : HOA
    embedding_dim : 16
    orders : 2
    layer_sizes : [256, 128, 64, 1]         # combined dnn
    layer_activations : [relu, relu, relu]  # combined dnn
    layer_dropout : 0.1

    # len must equal to orders
    cross_layer_heads : [2, 2]
    cross_layer_dims : [8, 8]                # only work if projection=True

    # only work if reduce=mlp_pooling
    reduce_layer_sizes : [256, 256]
    reduce_layer_activations : [0.3, 0.3]
#    load_model_name : ./checkpoint/epoch_1


#train
#init_method: normal,tnormal,uniform,he_normal,he_uniform,xavier_normal,xavier_uniform
train:
    init_method: tnormal
    init_value : 0.1
    embed_l2 : 0.001
    embed_l1 : 0.000
    layer_l2 : 0.001
    layer_l1 : 0.000
    learning_rate : 0.001
    loss : log_loss
    optimizer : adam
    epochs : 10
    batch_size : 2048

#show info
#metric :'auc','logloss', 'group_auc'
info:
    show_step : 20
    save_epoch : 2
    metrics : ['auc','logloss']

